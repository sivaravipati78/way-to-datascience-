{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incalss Laboratory - Day 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given data :\n",
    "\n",
    "* **The data set contains information about money spent on advertisement and their generated sales. Money was spent on TV, radio and newspaper ads.**\n",
    "\n",
    "**Note:** The objective is to use linear regression to understand how advertisement spending impacts sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Sl No | Variable | Description |\n",
    "| ---- | ---------------- | ------------------------------------- |\n",
    "| 1 | TV | float | \n",
    "| 2 | radio | float | \n",
    "| 3 | newspaper | float |  \n",
    "| 4 | sales | float  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt   \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\Sid\\Desktop\\Statistical Methods for Decision Making\\stats_regresion_residency\\Day04\\Faculty_NoteBook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Advertising.csv', index_col=0)\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Check the dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: Check whether data has numerical or categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5: visualize the relationship between the features and the response using scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, x_vars=['TV','radio','newspaper'], y_vars='sales', size=7, aspect=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6: Plot each independent variable using the regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, x_vars=['TV','radio','newspaper'], y_vars='sales', size=7, aspect=0.7, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's estimate the model coefficients for the advertising data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7: Create a fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1 = smf.ols(formula='sales ~ TV', data=data).fit()\n",
    "\n",
    "# print the coefficients\n",
    "lm1.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Here we don't need to add the add_constant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step8: Another way to fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step8.1: Create response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.sales \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step8.2: create predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.TV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step8.3 : Adds a constant term to the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)  \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATION:\n",
    "\n",
    "* Now we perform the regression of the predictor on the response, using the ``sm.OLS`` class and and its initialization ``OLS(y, X)`` method. This method takes as an input two array-like objects: ``X`` and ``y``. \n",
    "* In general, ``X`` will either be a numpy array or a pandas data frame with shape ``(n, p)`` where ``n`` is the number of data points and ``p`` is the number of predictors. \n",
    "* ``y`` is either a one-dimensional numpy array or a pandas series of length ``n``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step9 : Fitting Simple Linear Regression IN Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = sm.OLS(y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OBSERVATION:\n",
    "* We then need to fit the model by calling the OLS object's ``fit()`` method. Ignore the warning about the kurtosis test if it appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = est.fit()\n",
    "est.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step10: Check stats summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import formula api as alias smf\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# formula: response ~ predictors\n",
    "est = smf.ols(formula='sales ~ TV', data=data).fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Simple Linear Regression Modelling in Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Advertising.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Check the relation between dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data.TV,data.sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Plot all variables check the relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr(),annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "1.The diagonal of the above matrix shows the auto_correlation of the variance .Its is always 1. We can observe that \n",
    "the correlation between sles vs tv  is highest i.e 0.78.\n",
    "\n",
    "2.Correlaton can be vary from -1 and +1 .Closer to +1 means strong positive correlation and close -1 means\n",
    "strong negative correlation . Closer to 0 means not very strong correlation . Variables with strong \n",
    "correlations are mostly probabily candidtes for the model bulding.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Classifying the dependent and Independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['TV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[feature_cols]\n",
    "y = data.sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6:  Splitting the dataset into the Training set and Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 1/3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7: Fitting Simple Linear Regression to the Training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step8:  Intercept value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.intercept_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step9: coefficients value Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.coef_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictivebalance = regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 10: R^2 and MSE and RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ term is the coefficient of determination and it reflects how well the model fits the observed data.\n",
    "\n",
    "The coefficient of determination is given by $R^2$ = 1 - $\\frac{SSE}{SST}$\n",
    "\n",
    "* Let y be the observed response.\n",
    "* $\\hat{y}$ be the predicted value for the response\n",
    "* $\\overline{y}$ be the mean of the response\n",
    "\n",
    "SSE = $\\sum_{i=1}^{n}{(y_i - \\hat{y}_i)}^2$\n",
    "\n",
    "SST = $\\sum_{i=1}^{n}{(y_i - \\overline{y}_i)}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Issure with R-squared\n",
    "\n",
    "* R-squared will always increase as you add more features to the model, even if they are unrelated to the response\n",
    "Selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2:\",regressor.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = metrics.mean_absolute_error(predictivebalance,y_train)\n",
    "print(\"Mean Absolute Error:\", MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = metrics.mean_squared_error(predictivebalance,y_train)\n",
    "print(\"Mean Squared Error:\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_train, predictivebalance)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7: Visualising the Training set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step8: Visualising the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(X_test, y_test, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title('Salary vs Experience (Test set)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step9: Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "model1=sm.OLS(y_train,X_train)\n",
    "result = model1.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary packages for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have imported your packages, let's read the data that we are going to be using. \n",
    "\n",
    "**Note:** The dataset provided is a titled **housing_data.csv** and contains housing prices and information about the features of the houses. Below, read the data into a variable and visualize the top 8 rows of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('housing_Test.csv')\n",
    "data = pd.read_csv('housing_Train.csv')\n",
    "data.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test\n",
    "\n",
    "In the code below, we need to split the data into the train and test for modeling and validation of our models. We will cover the Train/Validation/Test as we go along in the project. Fill the following code.\n",
    "\n",
    "1\\. Subset the features to the variable: features <br>\n",
    "2\\. Subset the target variable: target <br>\n",
    "3\\. Set the test size in proportion in to a variable: test_size <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['lot_area', 'firstfloor_sqft', 'living_area', 'bath', 'garage_area', 'price']]\n",
    "target = data['price']\n",
    "test_size = .33\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "**NOTE:** The best way to explore the data we have is to build some plots that can help us determine the relationship of the data. We can use a scatter matrix to explore all our variables. Below is some starter code to build the scatter matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = pd.plotting.scatter_matrix(x_train, figsize=(14,8), alpha=1, diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a brief description of what you observe in scatter matrix above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Think about the relationship and whether linear regression is an appropriate choice for modelling this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. lot_area:\n",
    "\n",
    "\n",
    "---------\n",
    "1. Our initial intutions tell us that lot_area would be the best indicator of price; that being said, there is a weak correlation between lot_area and the other features, which is a good sign! However, the distribution is dramatically skewed-right indicating that the mean lot_area is greater than the median. \n",
    "\n",
    "2. This tells us that lot_area stays around the same size while price increases.\n",
    "\n",
    "3. In turn, that tells us that some other feature is helping determine the price bceause if lot_area we're determining the increase in price, we'd see a linear distribution. In determining the best feature for my linear regression model, So we think lot_area may be one of the least fitting to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. firstfloor_sqft:\n",
    "\n",
    "\n",
    "1. There is a stronger correlation between firstfloor_sqft and the other features. The distrubution is still skewed-right making the median a better measure of center. \n",
    "\n",
    "2. firstfloor_sqft would be a good candidate for the linear regression model becuse of the stronger correlation and wider distribution; however, there appears to be a overly strong, linear correlation between firstfloor_sqft and living_area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. living_area:\n",
    "\n",
    "------------\n",
    "1. There is a similarly strong correlation between living_area (as compared to firstfloor_sqft) and the other features, but these plots are better distributed than firstfloor_sqft. \n",
    "\n",
    "2. A right skew still exists, but less so than the firstfloor_sqft. However, the observation of a strong, linear correlation between firstfloor_sqft and living_area (or living_area and firstfloor_sqft) is reinforced here. \n",
    "\n",
    "3. Thus, we would not use both of these in my final model and having to choose between the two, we will likely choose living_area since it appears to be more well-distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. bath:\n",
    "\n",
    "----------\n",
    "1. Baths are static numbers, so the plots are much less distributed; however, the length and the clustering of the bath to living_area & bath to garage_area may indicate a correlation. Since we cannot use both living_area and firstfloor_sqft, and we think living_area has a better distribution, I would consider using bath in conjunction with living_area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. garage_area:\n",
    "\n",
    "------\n",
    "1. Garage_area appears to be well-distributed with the lowest correlation between the other features. This could make it a great fit for the final regression model. It's also the least skewed right distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix:\n",
    "\n",
    "**NOTE:** In the code below, compute the correlation matrix and We are writting a few thoughts about the observations. In doing so, consider the interplay in the features and how their correlation may affect your modeling.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas correlation function\n",
    "x_train.corr(method='pearson').style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are writting a few thoughts about the observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The correlation matrix above is in-line with our thought process. Lot_area has the lowest correlation between it and the other features, but it's not well distributed. \n",
    "------------\n",
    "2. firstfloor_sqft has a strong correlation between it and living_area. Given that the correlation is just over 0.5, both features may be able to be used in the model given that the correlation isn't overly strong; however, to be most accurate, We plan to leave out one of them (likely firstfloor_sqft). \n",
    "\n",
    "\n",
    "\n",
    "3. living_area also reflects this strong correlation between it and firstfloor_sqft. Surprisingly, there is a strong correlation between living_area and bath. \n",
    "--------------\n",
    "4. Looking solely at the scatter matrix, we did not see this strong correlation. This changes my approach slighltly, which we will outline above. garage_area, again, has the lowest correlations while being the most well-distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach: \n",
    "\n",
    "Given this new correlation information, We will approach the regression model in one of the following ways:\n",
    "\n",
    "        1. Leave out bath as a feature and use living_area + garage_area.\n",
    "        2. Swap firstfloor_sqft for living_area and include bath + garage area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We are not 100% sure if more features are better than less in this situation; however, We are sure that We want linearly independet features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Your Model\n",
    "\n",
    "Now that we have explored the data at a high level, let's build our model. In this section you will create your own estimators. Starter code is provided to makes this easier.\n",
    "\n",
    "\n",
    "$$\\beta_0 = \\bar {y} - \\beta_1 \\bar{x}$$ <br>\n",
    "$$\\beta_1 = \\frac {cov(x, y)} {var(x)}$$ <br>\n",
    "\n",
    "Below, let's define functions that will compute these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the necessary arguments in the function to calculate the coefficients\n",
    "\n",
    "def compute_estimators(feature, target):\n",
    "    n1 = np.sum(feature*target) - np.mean(target)*np.sum(feature)\n",
    "    d1 = np.sum(feature*feature) - np.mean(feature)*np.sum(feature)\n",
    "    \n",
    "    # Compute the Intercept and Slope\n",
    "    beta1 = n1/d1\n",
    "    beta0 = np.mean(target) - beta1*np.mean(feature)\n",
    "    \n",
    "    return beta0, beta1 # Return the Intercept and Slope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the compute estimators function above and display the estimated coefficients for any of the predictors/input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to pass the correct arguments\n",
    "beta0, beta1 = compute_estimators(data1['living_area'], data1['price'])\n",
    "print(beta0, beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Computing coefficients for our model by hand using the actual mathematical equations\n",
    "y = beta1 + beta0\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. sklearn solution\n",
    "\n",
    "**NOTE:** Now that we know how to compute the estimators, let's leverage the sklearn module to compute the metrics for us. We have already imported the linear model, let's initialize the model and compute the coefficients for the model with the input above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize the linear Regression model here\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# Pass in the correct inputs\n",
    "model.fit(data1[['living_area']], data1['price'])\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"This is beta0:\", model.intercept_)\n",
    "print(\"This is beta1:\", model.coef_) \n",
    "#### Computing coefficients for our model using the sklearn package \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the results from the cell above and your implementation match? They should be very close to each other. But they match!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Evaluation\n",
    "\n",
    "**NOTE:** Now that we have estimated our single model. We are going to compute the coefficients for all the inputs. We can use a for loop for multiple model estimation. However, we need to create a few functions:\n",
    "\n",
    "1\\. Prediction function: Functions to compute the predictions <br>\n",
    "2\\. MSE: Function to compute Mean Square Error <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that computes predictions of our model using the betas above + the feature data we've been using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predictions(intercept, slope, feature):\n",
    "    \"\"\" Compute Model Predictions \"\"\"\n",
    "    y_function = intercept+(slope*feature)\n",
    "    \n",
    "    return y_function\n",
    "\n",
    "y_function = model_predictions(beta0, beta1, data1['living_area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute MSE which determines the total loss for each predicted data point in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(y_outcome, predictions):\n",
    "    \"\"\" Compute the mean square error \"\"\"\n",
    "    mse = (np.sum((y_outcome - predictions) ** 2))/np.size(predictions)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "mse = mean_square_error(target, y_function)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last function we need is a plotting function to visualize our predictions relative to our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Function used to plot the data: \n",
    "\n",
    "def plotting_model(feature, target, predictions, name):\n",
    "    \"\"\" Create a scatter and predictions  \"\"\"\n",
    "    fig = plt.figure(figsize=(10,8)) \n",
    "    plot_model = model.fit(feature, target)\n",
    "    plt.scatter(x=feature, y=target, color='blue')\n",
    "    plt.plot(feature, predictions, color='red')\n",
    "    plt.xlabel(name)\n",
    "    plt.ylabel('Price')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = plotting_model(data1[['living_area']], data1['price'], y_function, data1['living_area'].name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integrity:\n",
    "\n",
    "-----------\n",
    "1. After our inital linear model based on the feature \"living area,\n",
    "-----------\n",
    "2. If we look at the graph above, there are 4 outliers that are clear, and at least 4 others that follow a similar trend based on the x, y relationship. \n",
    "\n",
    "\n",
    "3. We used ~3500 sqft of living area as my cutoff for being not predictive of the model, and any price above 600000. Given the way these data points skew the above model, they intuitively appear to be outliers with high leverage.\n",
    "\n",
    "-------------\n",
    "4. We determined this by comparing these high leverag points with points similar to it in someway and determined whether it was an outlier (i.e. if point A's price was abnormally high, I found a point (B) with living area at or close to point A's living area and compared the price. vice versa if living area was abnormally high)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inital Feature Analysis - \"Best\" Feature (a priori):\n",
    "-------------\n",
    "1. Living area is the best metric to use to train the linear model because it incorporates multiple of the other features within it: first floor living space & bath. \n",
    "\n",
    "------------\n",
    "2. Living area has a high correlation with both first floor sq ft (0.53) and baths (0.63). Based on the other correlations, these are the two highest, and thus should immediately be eliminated. \n",
    "\n",
    "---------\n",
    "3. Additionally, based on initial intuition, one would assume that an increase in the metric \"firstfloor sqft\" will lead to an increase in the \"living area\" metric; if both firstfloor sqft and overall living area are increased, the \"bath\" metric will likely also increase to accommodate the additional living area/sqft in a home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Each Single Feature through to determine which has best linear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = data[['living_area', 'garage_area', 'lot_area', 'firstfloor_sqft', 'bath']]\n",
    "count = 0\n",
    "\n",
    "for feature in features:\n",
    "    feature = features.iloc[:, count]\n",
    "    # Compute the Coefficients\n",
    "    beta0, beta1 = compute_estimators(feature, target)\n",
    "    count+=1\n",
    "    \n",
    "    # Print the Intercept and Slope\n",
    "    print(feature.name)\n",
    "    print('beta0:', beta0)\n",
    "    print('beta1:', beta1)\n",
    "\n",
    "    # Compute the Train and Test Predictions\n",
    "    y_hat = model_predictions(beta0, beta1, feature)\n",
    "\n",
    "    # Plot the Model Scatter  \n",
    "    name = feature.name\n",
    "    model = plotting_model(feature.values.reshape(-1, 1), target, y_hat, name)\n",
    "    \n",
    "    # Compute the MSE\n",
    "    mse = mean_square_error(target, y_hat)\n",
    "    print('mean squared error:', mse)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Feature Linear Models:\n",
    "------------\n",
    "1. MSE for Living Area drop significantly from 8957196059.803959 to 2815789647.7664313. In fact, Living Area has the lowest MSE 2815789647.7664313 of all the individual models, and the best linear fit.\n",
    "\n",
    "---------------\n",
    "2. Garage Area is the next lowest MSE 3466639234.8407283, and the model is mostly linear; however, the bottom left of the model is concerning. \n",
    "\n",
    "----------\n",
    "3. We'll notice that a large number of data points go vertically upward indicating an increase in price with 0 garage area. That says to us that garage area isn't predicting the price of these homes, which indicates that it may be a good feature to use in conjunction with another feature (i.e. Living Area) or since those data points do not fit in with the rest of the population, they may need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model \n",
    "\n",
    "**NOTE:** Now that we have our functions ready, we can build individual models, compute preductions, plot our model results and determine our MSE. **Notice that we compute our MSE on the test set and not the train set.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product (multiple feature) Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models Living Area alone and compares it to the Dot Product of Living Area with each other feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining if a MLR would be a better way to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = data[['living_area', 'garage_area', 'lot_area', 'firstfloor_sqft', 'bath']]\n",
    "count = 0\n",
    "\n",
    "for feature in features:\n",
    "    feature = features.iloc[:, count]\n",
    "    #print(feature.head(0))\n",
    "    if feature.name == 'living_area':\n",
    "        x = data['living_area']\n",
    "    else:\n",
    "        x = feature * data['living_area']\n",
    "    # Compute the Coefficients\n",
    "    beta0, beta1 = compute_estimators(x, target)\n",
    "    \n",
    "    # Print the Intercept and Slope\n",
    "    if feature.name == 'living_area':\n",
    "        print('living_area')\n",
    "        print('beta0:', beta0)\n",
    "        print('beta1:', beta1)\n",
    "    else: \n",
    "        print(feature.name, \"* living_area\") \n",
    "        print('beta0:', beta0)\n",
    "        print('beta1:', beta1)\n",
    "\n",
    "    # Compute the Train and Test Predictions\n",
    "    y_hat = model_predictions(beta0, beta1, x)\n",
    "\n",
    "    # Plot the Model Scatter  \n",
    "    if feature.name == 'living_area':\n",
    "        name = 'living_area'\n",
    "    else:\n",
    "        name = feature.name + \" \" + \"* living_area\" \n",
    "    model = plotting_model(x.values.reshape(-1, 1), target, y_hat, name)\n",
    "    \n",
    "    # Compute the MSE\n",
    "    mse = mean_square_error(target, y_hat)\n",
    "    print('mean squared error:', mse)\n",
    "    print()\n",
    "    count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis:\n",
    "\n",
    "Based on the models, it appears that two of the dot products provide a more accurate model:\n",
    "    \n",
    "    1. Living Area * First Floor SqFt\n",
    "    2. Living Area * Garage Area \n",
    "    \n",
    "These two dot products provide a lower MSE and thus lowers the loss per prediction point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Living Area * First Floor SqFt:\n",
    "\n",
    "\n",
    "\n",
    "* Our intuition says that since Living Area, as a feature, will include First Floor SqFt in its data. The FirstFloor SqFt can be captured by Living Area, so it can be left out. \n",
    "\n",
    "* Additionally, since one is included within the other, we cannot say anything in particular about Living Area or FirstFloor SqFt individually. \n",
    "\n",
    "* Also, the correlation (Ln 24 & Out 24) between Living Area and FirstFloor SqFt is 0.53, which is the highest apart from Bath. \n",
    "\n",
    "* This correlation is low in comparison to the \"standard;\" however, that standard is arbitrary. I've lowered it to be in context with data sets I'm working with in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.Living Area * Garage Area:\n",
    "\n",
    "* The dot product of Living Area & Garage Area provides doesn't allow us to make a statement about each individually, unless we provide a model of each, which we will do below. \n",
    "\n",
    "* This dot product is a better model. Garage Area is advertised as 'bonus' space and CANNOT be included in the overall square footage of the home (i.e. living area). \n",
    "\n",
    "* Thus, garage area vector will not be included as an implication within the living area vector making them linearly independent. \n",
    "\n",
    "* Garage Area can be a sought after feature depending on a buyer's desired lifestlye; more garage space would be sought after by buyers with more cars, which allows us to draw a couple possible inferences about the buyers:\n",
    "\n",
    "    1. enough net worth/monthly to make payments on multiple vehicles plus make payments on a house/garage\n",
    "    2. enough disposable income to outright buy multiple vehicles plus make payments on a house/garage\n",
    "\n",
    "**Additionally,** it stands to reason that garage area would scale with living area for pragmatic reasons (more living area implies more people and potentially more vehicles) and for aesthetic reasons (more living area makes home look larger and would need larger garage).\n",
    "\n",
    "* Homes with more living area and garage area may be sought after by buyers with the ability to spend more on a home, and thus the market would bear a higher price for those homes, which helps explain why living area * garage area is a better indicator of home price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Combining living area with other features lowered the MSE for each. The lowest MSE is living area * garage area, which **confirms my hypothesis:** Living Area is the best feature to predict price, and garage area is good when used in conjunction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling Living Area & Garage Area separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = data[['living_area', 'garage_area']]\n",
    "count = 0\n",
    "for feature in features:\n",
    "    feature = features.iloc[:, count]\n",
    "    if feature.name == 'living_area':\n",
    "        x = data['living_area']\n",
    "    elif feature.name == 'garage_area':\n",
    "        x = data['garage_area']\n",
    "    \n",
    "    beta0, beta1 = compute_estimators(x, target)\n",
    "    count+=1\n",
    "    \n",
    "    if feature.name == 'living_area':\n",
    "        print('living_area')\n",
    "        print('beta0:', beta0)\n",
    "        print('beta1:', beta1)\n",
    "    elif feature.name == 'garage_area':\n",
    "        print('garage_area')\n",
    "        print('beta0:', beta0)\n",
    "        print('beta1:', beta1)\n",
    "    \n",
    "    y_hat = model_predictions(beta0, beta1, x)\n",
    "    \n",
    "    if feature.name == 'living_area':\n",
    "        name = 'living_area'\n",
    "    elif feature.name == 'garage_area':\n",
    "        name = 'garage_area'\n",
    "    model = plotting_model(x.values.reshape(-1, 1), target, y_hat, name)\n",
    "    \n",
    "    mse = mean_square_error(target, y_hat)\n",
    "    print('mean squared error:', mse)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling dot product of Living Area * Garage Area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = data[['living_area']]\n",
    "x = features.iloc[:, 0]\n",
    "x2 = x * data['garage_area']\n",
    "#x3 = x2 * data['bath']\n",
    "\n",
    "# Compute the Coefficients\n",
    "beta0, beta1 = compute_estimators(x2, target)\n",
    "\n",
    "# Print the Intercept and Slope\n",
    "print('Name: garage_area * living_area')\n",
    "print('beta0:', beta0)\n",
    "print('beta1:', beta1)\n",
    "\n",
    "# Compute the Train and Test Predictions\n",
    "y_hat_1 = model_predictions(beta0, beta1, x2)\n",
    "\n",
    "# Plot the Model Scatter  \n",
    "name = 'garage_area * living_area'\n",
    "model = plotting_model(x2.values.reshape(-1, 1), target, y_hat_1, name)\n",
    "\n",
    "# Compute the MSE\n",
    "mse = mean_square_error(target, y_hat_1)\n",
    "print('mean squared error:', mse)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning:\n",
    "\n",
    "* Above, We modeled both living area and garage area by themselves then the dot product of Living Area * Garage Area to highlight the MSE of each vs. \n",
    "* The MSE of the dot product. Garage Area, much more so than Living Area, has a high MSE indicating that on its own, Garage Area isn't the best predictor of a home's price; we must take the data in context with reality, and intuitively speaking, one wouldn't assume that the garage area, on its own, would be a feature indicative of price. \n",
    "\n",
    "* This fact combined with the assumption/implication that garage may scale with living area implies some correlation between the features, which would go against the linear assumption of feature independence. As a matter of fact, there is a correlation between them (Ln 24 & Out 24) of 0.44; however, this isn't problematic for two reasons:\n",
    "\n",
    "    1. 0.44 is quite low in regard to typical correlation standards.\n",
    "    2. Data must be seen in context. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  0.44 is quite low in regard to typical correlation standards:\n",
    "\n",
    "* Although We eliminated First Floor SqFt due, in part, to a high correlation and that correclation is only 0.09 points lower. \n",
    "* The main reason why First Floor SqFt is eliminated is due to its inclusion within the living area vector. Additionally, the main reason why I'm including garage area is because it is not included with the living area vector.\n",
    "--------------\n",
    "####  Data must be seen in context:\n",
    "\n",
    "* Similar to our #1 explanation, knowing that garage area is 'bonus space' and, as such, is NOT included in a home's advertised square feet indicates that it isn't within the Living Area data set in the same way FF SqFt or Baths would be.\n",
    "\n",
    "* It will most likely to scale with the living area independently of the living area making it a good fit for a MLR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Interpretation\n",
    "\n",
    "**NOTE:** Now that you have calculated all the individual models in the dataset, provide an analytics rationale for which model has performed best. To provide some additional assessment metrics, let's create a function to compute the R-Squared.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$R^2 = \\frac {SS_{Regression}}{SS_{Total}} = 1 - \\frac {SS_{Error}}{SS_{Total}}$$<br>\n",
    "\n",
    "where:<br>\n",
    "$SS_{Regression} = \\sum (\\widehat {y_i} - \\bar {y_i})^2$<br>\n",
    "$SS_{Total} = \\sum ({y_i} - \\bar {y_i})^2$<br>\n",
    "$SS_{Error} = \\sum ({y_i} - \\widehat {y_i})^2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssr = sum of squares of regression --> variance of prediction from the mean\n",
    "#sst = sum of squares total --> variance of the actuals from the prediction\n",
    "#sse = sume of squares error --> variance of the atuals from the mean\n",
    "def r_squared(y_outcome, predictions):\n",
    "    \"\"\" Compute the R Squared \"\"\"\n",
    "    ssr = np.sum((predictions - np.mean(y_outcome))**2)\n",
    "    sst = np.sum((y_outcome - np.mean(y_outcome))**2)\n",
    "    sse = np.sum((y_outcome - predictions)**2)\n",
    "    \n",
    "#    print(sse, \"/\", sst)\n",
    "    print(\"1 - SSE/SST =\", round((1 - (sse/sst))*100), \"%\")\n",
    "    \n",
    "    rss = (ssr/sst) * 100\n",
    "    \n",
    "    return rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Now that you we have R Squared calculated, evaluate the R Squared for the test group across all models and determine what model explains the data best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss = r_squared(target, y_hat_1)\n",
    "print(\"R-Squared =\", round(rss), \"%\")\n",
    "count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Squared Adjusted\n",
    "\n",
    "$R^2-adjusted = 1 - \\frac {(1-R^2)(n-1)}{n-k-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared_adjusted(rss, sample_size, regressors):\n",
    "    n = np.size(sample_size)\n",
    "    k = regressors\n",
    "    numerator = (1-rss)*(n)\n",
    "    denominator = n-k-1\n",
    "    rssAdj = 1 - (numerator / denominator)\n",
    "    \n",
    "    return rssAdj\n",
    "\n",
    "rssAdj = r_squared_adjusted(rss, y_hat_1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(rssAdj), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "For the data given in the data set \"C:\\Users\\Sid\\Desktop\\SM-OCT19\\Day4\\Inclass Lab\\slr example.xlsx\", perform simple linear regression analysis using y as response variable and x as the regressor variable. Your analysis should include exploratory data analysis, model fitting and model validation and final recommendations if any.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "For the caverns data given in the excel file, \"C:\\Users\\Sid\\Desktop\\SM-OCT19\\Day4\\Inclass Lab\\caverns.xlsx\" perform multiple linear regression analysis using potential as response variable and all others as the regressor variables. The description of variables follows:\n",
    "\n",
    "Potential: Sales potential for the salesperson.\n",
    "Ad Exp: amount spent on product and brand promotions\n",
    "Share: Share of company products in the territory\n",
    "Change: Change (expressed as a fraction) in the market share since last year\n",
    "Account: Average number of accounts held by the sales person. Fractions indicate part of the year.\n",
    "Work: Number of months since employment in the company.\n",
    "Rating: Preformnce rating of the sales person\n",
    "\n",
    "Your analysis should include exploratory data analysis, model fitting and model validation and final recommendations if any. Address any multi-collinearity issues and suggest the best model using max R approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################  End of Lab  #####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
